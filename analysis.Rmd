---
title: "Activity Tracker Prediction"
author: "Justin Owens"
date: "April 14, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#trace(utils:::unpackPkgZip, edit=TRUE)
library(data.table)
library(caret)
library(corrplot)
library(plotly)
library(ggplot2)
library(dplyr)
library(magrittr)
library(parallel)
library(doParallel)
set.seed(666)
```

#Project Description
***
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset). A huge shout out to the team that allowed us to use this dataset.

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

#Pre-processing Data
***
The first section will consider loading and pre-processing the data, a critical step for creating accurate and useful models.

##Load Data
First, the data that will be used to create prediction models for fitness activity manner must be **loaded** into the R session. The .csv files are loaded as-is considering blank strings, 'NA' strings, and '#DIV/0!' as NA value types.
```{r load_data}
if(!file.exists('./training_data.csv')){
  download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', './training_data.csv')
  base_data = fread('./training_data.csv', sep = ',', na.strings = c('','NA','#DIV/0!'))
} else {
  base_data = fread('./training_data.csv', sep = ',', na.strings = c('','NA','#DIV/0!'))
}
if(!file.exists('./testing_data.csv')){
  download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', './testing_data.csv')
  eval_data = fread('./testing_data.csv', sep = ',', na.strings = c('','NA','#DIV/0!'))
} else {
  eval_data = fread('./testing_data.csv', sep = ',', na.strings = c('','NA','#DIV/0!'))
}
```

##Clean Data
Next, the data must be in a **clean** and proper format. Most features have been converted to numeric type. Other factor variables such as username and timestamp are removed for the purposes of these models. Many features also contain very little data, so these columns have been omitted and the final dataset only includes columns where the majority of data are available.
```{r clean_data, echo=TRUE}
#remove unnecessary columns
remove_col = c('V1', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
base_data[,(remove_col):=NULL]
#identify factor columns
fact_col = c('classe')
base_data[, (fact_col) := lapply(.SD, as.factor), .SDcols = fact_col]
#identify numeric columns
num_col = names(base_data)[-which(names(base_data) %in% fact_col)]
base_data[, (num_col) := lapply(.SD, as.numeric), .SDcols = num_col]
#clean out features with minimal data
sig_na = names(base_data)[colSums(is.na(base_data))/nrow(base_data) >= .9]
base_data[,(sig_na):=NULL]
```
The final cleaned dataset contains `r  dim(base_data)[1]` observations of `r  dim(base_data)[2]` features.


##Split into Training and Test Datasets
In order to properly train the model while mitigating bias and overfitting, the data must be also split into training and testing datasets. The training data to create the models and the testing data will help understand out of sample accuracy and model fit.
```{r split_data, echo=TRUE}
inTrain = createDataPartition(base_data$classe, p = 3/4)[[1]]
training = base_data[ inTrain,]
testing = base_data[-inTrain,]
```
The training data is `r (round(dim(training)[1]/(dim(training)[1] + dim(testing)[1]),4)*100)`% of the superset.

#Configure and Create Models
***
The next section will consider setup and execution for creating models efficiently using parallel processing and the caret package.

##Preparing Parallel Execution
To help make the actual computation more efficient -- especially for random forest models -- the parallel processing package can be utilized.
```{r parallel_start, echo = TRUE}
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

##Configure the trainControl object
To allow for better cross validation, the trainControl object will be initialized with 4 folds.
```{r trainControl, echo = TRUE}
fitControl = trainControl(method = "cv", number = 4, allowParallel = TRUE)
```

##Create the models
A few different types of models using the final dataset and machine learning configurations will be created:
* rpart
* random forest
* boosting

```{r model_create, echo=TRUE, cache=TRUE, results='hide'}
#rpart model
rpart_model = train(classe ~ ., method = "rpart", data = training, trControl = fitControl, na.action = na.pass)
#random forest model
rf_model = train(classe ~ ., method = "rf", data = training, trControl = fitControl, na.action = na.pass)
#boosting model
gbm_model = train(classe ~ ., method = "gbm", data = training, trControl = fitControl, na.action = na.pass)
```

##De-register Parallel Processing
After utilizing all but one core for parallel processing, one should halt the cluster to force R to run in single-threaded processing.
```{r parallel_stop, echo = TRUE}
stopCluster(cluster)
registerDoSEQ()
```

#Evaluating the Models
***
The final section will review the model evaluation to understand what model was best chosen from this exercise.

##Predicting from Created Models
Using the created models, one can predict the classification on the training and test sets and compare against the observed classification to understand in-sample and out-of-sample error.

###Evaluation Using Training Dataset
One can evaluate the in-sample error rate using the training dataset used to create the model.

####Predict
First, predict the values of **classe** using the created models and the original training dataset.
```{r model_predict_train, echo=TRUE}
rpart_pred_train = predict(rpart_model$finalModel, training, type = 'class')
rf_pred_train = predict(rf_model$finalModel, training)
gbm_pred_train = predict(gbm_model, training)
```

####Calculate Accuracy
Next, grab the accuracy of the model. The in-sample error rate is simply 1 - the accuracy rate. However, the in-sample error rate will always be higher than the out-of-sample error rate.
```{r model_acc_train, echo=TRUE}
confusionMatrix(rpart_pred_train, training$classe)$overall[1]
confusionMatrix(rf_pred_train, training$classe)$overall[1]
confusionMatrix(gbm_pred_train, training$classe)$overall[1]
```
In this case, the Random Forest model has a phenomenally perfect accuracy rate of 100%, but one should further evaluate on the testing dataset to estimate out-of-sample error rate. As a note, this model could possibly be influenced by overfitting with such an accuracy rate.

###Evaluation Using Testing Dataset
As mentioned previously, it is a better idea to validate models using data not used to create the model. The testing dataset was set aside before creating the models specifically for this purpose and can be used to estimate error rate.

####Predict
First, predict the values of **classe** using the created models and the testing dataset set aside before creating the models.
```{r model_predict_test, echo=TRUE}
rpart_pred_test = predict(rpart_model$finalModel, testing, type = 'class')
rf_pred_test = predict(rf_model$finalModel, testing)
gbm_pred_test = predict(gbm_model, testing)
```

####Calculate Accuracy
Grab the out-of-sample accuracy using the confusionMatrix function and the actual observed class values from the dataset.
```{r model_acc_test, echo=TRUE}
confusionMatrix(rpart_pred_test, testing$classe)$overall[1]
confusionMatrix(rf_pred_test, testing$classe)$overall[1]
confusionMatrix(gbm_pred_test, testing$classe)$overall[1]
```
In either case, the Random Forest and Boosting algorithms provided extremely accurate models to predict class from the set of predictors chosen.

###Out-of-sample Error
Because the accuracy rate was highest for the Random Forest model, it is worth mentioning the estimated error rate we expect to see with any new data.

This is about 1 - the accuracy rate or `r (round(confusionMatrix(rf_pred_test, testing$classe)$overall[1],4)*100)`%.

Another way to understand OOB error rate is by looking at the output of rf_model$finalModel -- the OOB estimate of error rate is 0.65% for the Random Forest model.

###Predict on Test Data
Now that we have evaluated the model, we can test using the 20 observations provided.
```{r clean_data_test, echo=FALSE}
#remove unnecessary columns
remove_col = names(eval_data)[-which(names(eval_data) %in% names(base_data))]
eval_data[,(remove_col):=NULL]
```

```{r test_pred, echo=TRUE}
predict.train(rf_model, eval_data)
```
